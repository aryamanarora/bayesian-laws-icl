<div align="center">

# Bayesian scaling laws for in-context learning

**Bayesian scaling laws for in-context learning**\
(Arora et al., 2024)

</div>

This repo implements theoretically-motivated Bayesian scaling laws for in-context learning. We include a code for pretraining and finetuning (SFT or DPO) small LMs on GINC in order to collect their ICL curves (adapted from [p-lambda/incontext-learning](https://github.com/p-lambda/incontext-learning)), running inference on real-world LLMs locally or through Together API in order to collect their ICL curves, and code for fitting and analysing our Bayesian scaling laws as well as some baselines. We also include a script for replicating all figures and tables in the paper.

## Instructions

TBD